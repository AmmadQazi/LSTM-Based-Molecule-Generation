{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d44c455-4401-410f-b471-75456a6990c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load pre-trained PyTorch model\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased').cuda()\n",
    "\n",
    "# Data collator used for dynamic padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # True if you're doing masked language modeling\n",
    "    mlm_probability=0.15  # Probability of masking a token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc367bf-e3a1-4cef-940e-8ea1925e0425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a051403-6db1-4d21-b040-27e6d883c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(issubclass(type(model), DistilBertForMaskedLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2685af1-6665-4572-86f8-f06a0d4604ac",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "# Normalization: \n",
    "Converts SMILES to a consistent, canonical format using RDKit, which helps in removing duplicates and maintaining consistency.\n",
    "# Tokenization: \n",
    "Splits the SMILES strings into individual characters and converts these into numerical tokens. This step is crucial for the model to learn from the sequence data.\n",
    "# Padding: \n",
    "Ensures that all input sequences to the model are of the same length by padding shorter sequences with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324e0070-b899-4b3f-a0bf-8d89340773af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of molecules: 1036643\n",
      "Number of valid molecules after normalization: 1036643\n",
      "Max length of SMILES: 96\n",
      "Vocabulary size: 30522\n",
      "Example of normalized SMILES: COc1ccc(N2CCN(C(=O)c3cc4ccccc4[nH]3)CC2)cc1\n",
      "Example of tokenized and padded SMILES: [101, 2522, 2278, 2487, 9468, 2278, 1006, 1050, 2475, 9468, 2078, 1006, 1039, 1006, 1027, 1051, 1007, 1039, 2509, 9468, 2549, 9468, 9468, 2278, 2549, 1031, 18699, 1033, 1017, 1007, 10507, 2475, 1007, 10507, 2487, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from transformers import DistilBertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to read SMILES from a file\n",
    "def read_smiles(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        smiles = file.read().strip().split('\\n')\n",
    "    return smiles\n",
    "\n",
    "# Normalize SMILES\n",
    "def normalize_smiles(smiles_list):\n",
    "    normalized = [Chem.MolToSmiles(Chem.MolFromSmiles(smile), canonical=True) for smile in smiles_list if Chem.MolFromSmiles(smile) is not None]\n",
    "    return normalized\n",
    "\n",
    "# Tokenize SMILES\n",
    "def tokenize_smiles(smiles_list):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    tokenized = [tokenizer.encode(smile, add_special_tokens=True) for smile in smiles_list]\n",
    "    return tokenized, tokenizer\n",
    "\n",
    "# Load and process the data\n",
    "file_path = 'smiles_train.txt'\n",
    "smiles = read_smiles(file_path)\n",
    "normalized_smiles = normalize_smiles(smiles)\n",
    "tokenized_smiles, tokenizer = tokenize_smiles(normalized_smiles)\n",
    "max_length = max(len(s) for s in tokenized_smiles)\n",
    "padded_smiles = [s + [0] * (max_length - len(s)) for s in tokenized_smiles]  # Padding manually\n",
    "\n",
    "# Insights on the data\n",
    "print(f\"Total number of molecules: {len(smiles)}\")\n",
    "print(f\"Number of valid molecules after normalization: {len(normalized_smiles)}\")\n",
    "print(f\"Max length of SMILES: {max_length}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")  # Vocabulary size is determined by the tokenizer itself\n",
    "\n",
    "# Example output\n",
    "print(\"Example of normalized SMILES:\", normalized_smiles[0])\n",
    "print(\"Example of tokenized and padded SMILES:\", padded_smiles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f897295-7d12-406f-95cc-1eed4cbec7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # PyTorch will use GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))  # This will print the name of the GPU\n",
    "else:\n",
    "    # PyTorch will use CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68f30a-8027-4bbf-8a00-135fb6152034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fed17-a05b-428d-b6e8-49f80bca7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(issubclass(type(model), DistilBertForMaskedLM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d59ef456-ad7f-449d-897b-cab40b57ccf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12959' max='12959' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12959/12959 4:58:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.240668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12959, training_loss=0.38036279680552476, metrics={'train_runtime': 17914.2631, 'train_samples_per_second': 46.294, 'train_steps_per_second': 0.723, 'total_flos': 2.061277923873715e+16, 'train_loss': 0.38036279680552476, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "# Convert lists to tensors and create a dataset\n",
    "input_ids = torch.tensor(padded_smiles, dtype=torch.long)\n",
    "\n",
    "attention_mask = torch.tensor([[1 if token > 0 else 0 for token in seq] for seq in padded_smiles], dtype=torch.long)\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_inputs, eval_inputs, train_masks, eval_masks = train_test_split(input_ids, attention_mask, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = SmilesDataset(train_inputs, train_masks)\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_dataset = SmilesDataset(eval_inputs, eval_masks)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./model_output',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,   # Load the best model at the end of training\n",
    "    evaluation_strategy=\"epoch\",   # Evaluate at each specified number of steps\n",
    "    save_strategy=\"epoch\",         # Save strategy to match evaluation\n",
    "    save_steps=5000,                # Number of steps to save the model\n",
    "    report_to=\"all\"                # Report to all available platforms\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd61a004-8ddb-4195-bf9e-dbeda4fe539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SMILES have been saved to 'generated_molecules.txt'.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_smiles(model, start_sequence, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(start_sequence, return_tensors='pt').to(device)\n",
    "        \n",
    "        generated_ids = input_ids\n",
    "        for _ in range(max_length):\n",
    "            masked_input_ids = generated_ids.clone()\n",
    "            masked_input_ids[:, -1] = tokenizer.mask_token_id\n",
    "            \n",
    "            outputs = model(masked_input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "            \n",
    "            if next_token_id.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "            \n",
    "        generated_smiles = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return generated_smiles\n",
    "\n",
    "num_sequences = 10\n",
    "generated_smiles_list = [generate_smiles(model, \"Molecule Start Sequence \", max_length=100) for _ in range(num_sequences)]\n",
    "\n",
    "with open('generated_molecules.txt', 'w') as f:\n",
    "    for smile in generated_smiles_list:\n",
    "        f.write(smile + '\\n')\n",
    "\n",
    "print(\"Generated SMILES have been saved to 'generated_molecules.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be8d5d-3d5d-4501-b107-055b8cb4dc64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
