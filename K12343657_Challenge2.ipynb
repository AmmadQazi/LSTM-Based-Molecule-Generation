{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324e0070-b899-4b3f-a0bf-8d89340773af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of molecules: 1036643\n",
      "Number of valid molecules after normalization: 1036643\n",
      "Max length of SMILES: 101\n",
      "Vocabulary size: 37\n",
      "Example of normalized SMILES: COc1ccc(N2CCN(C(=O)c3cc4ccccc4[nH]3)CC2)cc1\n",
      "Example of tokenized and padded SMILES: [19, 23, 29, 8, 29, 29, 29, 3, 22, 9, 19, 19, 22, 3, 19, 3, 17, 23, 4, 29, 10, 29, 29, 11, 29, 29, 29, 29, 29, 11, 26, 33, 20, 27, 10, 4, 19, 19, 9, 4, 29, 29, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "\n",
    "def read_smiles(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        smiles = file.read().strip().split('\\n')\n",
    "    return smiles\n",
    "\n",
    "def normalize_smiles(smiles_list):\n",
    "    normalized = [Chem.MolToSmiles(Chem.MolFromSmiles(smile), canonical=True) for smile in smiles_list if Chem.MolFromSmiles(smile) is not None]\n",
    "    return normalized\n",
    "\n",
    "def create_char_tokenizer(smiles_list):\n",
    "    all_chars = set(''.join(smiles_list))\n",
    "    char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(all_chars))}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "def tokenize_and_pad(smiles_list, char_to_idx):\n",
    "    max_length = max(len(smile) for smile in smiles_list)\n",
    "    tokenized = [[char_to_idx[char] for char in smile] for smile in smiles_list]\n",
    "    padded_smiles = [s + [0] * (max_length - len(s)) for s in tokenized]\n",
    "    return padded_smiles, max_length\n",
    "\n",
    "file_path = 'smiles_train.txt'\n",
    "smiles = read_smiles(file_path)\n",
    "normalized_smiles = normalize_smiles(smiles)\n",
    "char_to_idx, idx_to_char = create_char_tokenizer(normalized_smiles)\n",
    "padded_smiles, max_length = tokenize_and_pad(normalized_smiles, char_to_idx)\n",
    "\n",
    "print(f\"Total number of molecules: {len(smiles)}\")\n",
    "print(f\"Number of valid molecules after normalization: {len(normalized_smiles)}\")\n",
    "print(f\"Max length of SMILES: {max_length}\")\n",
    "print(f\"Vocabulary size: {len(char_to_idx)}\")  # Vocabulary size based on unique characters\n",
    "\n",
    "print(\"Example of normalized SMILES:\", normalized_smiles[0])\n",
    "print(\"Example of tokenized and padded SMILES:\", padded_smiles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59ef456-ad7f-449d-897b-cab40b57ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles, labels):\n",
    "        self.smiles = smiles\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.smiles[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a230ba-81af-445a-a984-013f8a1dab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SMILESModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd61a004-8ddb-4195-bf9e-dbeda4fe539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_idx) + 1\n",
    "embedding_dim = 64\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "input_seqs = [smile[:-1] for smile in padded_smiles]\n",
    "target_seqs = [smile[1:] for smile in padded_smiles]\n",
    "\n",
    "train_inputs, train_targets = input_seqs[:int(len(input_seqs) * 0.8)], target_seqs[:int(len(input_seqs) * 0.8)]\n",
    "val_inputs, val_targets = input_seqs[int(len(input_seqs) * 0.8):], target_seqs[int(len(input_seqs) * 0.8):]\n",
    "\n",
    "train_dataset = SMILESDataset(train_inputs, train_targets)\n",
    "val_dataset = SMILESDataset(val_inputs, val_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01d1da3-c2fe-40cd-8df2-6b39510a91f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c933e4185d47aeab2af228a398a9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d71646a7865412d8f116dbca51bb238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.32578375104224144, Average Validation Loss: 0.29426086135898477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b74eee6ba24de6b6f0cd99f24238aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b15bdc4eba840d1b3295380c17e25ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.28446685165257696, Average Validation Loss: 0.2801362621126535\n",
      "Checkpoint saved to model_checkpoints\\checkpoint_epoch_2.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f289a0e7a594b74a646337ac853e25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2dea94fe57468399fdf70d1bb08133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2759321748361721, Average Validation Loss: 0.27569945202335533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f07863135843829f14dc914602972e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b38c2ce4ba43d4a7a047c13edb4539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2717639230849746, Average Validation Loss: 0.2736120587789718\n",
      "Checkpoint saved to model_checkpoints\\checkpoint_epoch_4.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66694039ee3742598ec74a57bd4185d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbb6851a61641b3aaf5402be583fab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.26914192558257133, Average Validation Loss: 0.27150725125576614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e9a821b92346b2a0e671e0d470d32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98615463d6744f3eaa4799cb2bb19f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2674216797462735, Average Validation Loss: 0.270737173281794\n",
      "Checkpoint saved to model_checkpoints\\checkpoint_epoch_6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d5f703cffe49cd8a024823bf88ba20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06199c6a17c14efe8ef6ca8b6c1684a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.26622077717468107, Average Validation Loss: 0.2696965642733338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bd1a9d6b5e4cc9a52f4470d38c28a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354746ebb84742d19557956530c136aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2656387705932996, Average Validation Loss: 0.26978199839775946\n",
      "Checkpoint saved to model_checkpoints\\checkpoint_epoch_8.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41f375f8d5746e48c87f3110633d937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e640f5a4d24db5999526ea7127e73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.26480283617292577, Average Validation Loss: 0.2688694923104327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a293a9bc0a6943378b5444298235d6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f410a08a8d4584ba17efa9d1596c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.26434855683088515, Average Validation Loss: 0.26837995204108733\n",
      "Checkpoint saved to model_checkpoints\\checkpoint_epoch_10.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa29f4e4444645d3be6913501758df89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a23efc6319427db56b95a4f3083260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2638689709376897, Average Validation Loss: 0.26812668766274494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208dd5373e5e4b349354771061fb5d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/12 [Train]:   0%|          | 0/25917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c35785085e041aa932b650ae0a49fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/12 [Validate]:   0%|          | 0/6480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.26357746301986795, Average Validation Loss: 0.2680020926060316\n",
      "Checkpoint saved to model_checkpoints\\checkpoint_epoch_12.pth\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SMILESModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import os\n",
    "model_save_path = 'model_checkpoints'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "epochs = 12\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", mininterval=1.0)\n",
    "    for inputs, labels in train_pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.transpose(1, 2), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validate]\", mininterval=1.0)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "            valid_loss += loss.item()\n",
    "            val_pbar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = valid_loss / len(val_loader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss}, Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint_filename = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "        checkpoint_filepath = os.path.join(model_save_path, checkpoint_filename)\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': valid_loss,\n",
    "        }, checkpoint_filepath)\n",
    "        print(f\"Checkpoint saved to {checkpoint_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b9c93d-d8e1-4205-8dce-b43b72aeb614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SMILESModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
    "model_save_path = 'model_checkpoints'\n",
    "\n",
    "checkpoint_filename = \"checkpoint_epoch_12.pth\"\n",
    "checkpoint_filepath = os.path.join(model_save_path, checkpoint_filename)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_filepath)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(\"Model loaded from checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11be8d5d-3d5d-4501-b107-055b8cb4dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f01624fb9e44eca947999057e40a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating SMILES:   0%|          | 0/11200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_smiles(model, char_to_idx, idx_to_char, seed='C', max_length=100):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    current_sequence = seed\n",
    "    input_sequence = [char_to_idx[char] for char in current_sequence]\n",
    "    input_tensor = torch.tensor([input_sequence], dtype=torch.long).to(device)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probabilities = F.softmax(outputs[:, -1, :], dim=-1).squeeze()\n",
    "            predicted_idx = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "            if predicted_idx not in idx_to_char:\n",
    "                break \n",
    "\n",
    "            predicted_char = idx_to_char[predicted_idx]\n",
    "\n",
    "            if predicted_char == '\\n':\n",
    "                break\n",
    "\n",
    "            current_sequence += predicted_char\n",
    "            input_sequence.append(predicted_idx)\n",
    "            input_tensor = torch.tensor([input_sequence], dtype=torch.long).to(device)\n",
    "\n",
    "    return current_sequence\n",
    "\n",
    "\n",
    "def generate_large_number_of_smiles(model, num_smiles, seed_initial='C'):\n",
    "    generated_smiles = []\n",
    "    for _ in tqdm(range(num_smiles), desc=\"Generating SMILES\"):\n",
    "        smiles = generate_smiles(model, char_to_idx, idx_to_char, seed=seed_initial, max_length=100)\n",
    "        generated_smiles.append(smiles)\n",
    "    return generated_smiles\n",
    "\n",
    "num_generation = 11200\n",
    "all_generated_smiles = generate_large_number_of_smiles(model, num_generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8fb8d062-4c5b-4231-bbbd-bad94efaab8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edad3aec490e4db3994097c551f4cdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating and Scoring SMILES:   0%|          | 0/11200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "from tqdm.notebook import tqdm\n",
    "from rdkit import RDLogger\n",
    "\n",
    "def validate_and_score_smiles(smiles_list):\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    valid_smiles_scores = []\n",
    "    failed_smiles = []\n",
    "\n",
    "    for smiles in tqdm(smiles_list, desc=\"Validating and Scoring SMILES\"):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "                qed_score = QED.qed(mol)\n",
    "                valid_smiles_scores.append((smiles, qed_score))\n",
    "        except (Chem.KekulizeException, ValueError) as e:\n",
    "            print(f\"Failed to process SMILES {smiles}: {str(e)}\")\n",
    "            failed_smiles.append(smiles)\n",
    "\n",
    "    return valid_smiles_scores, failed_smiles\n",
    "\n",
    "validated_smiles, failed_smiles = validate_and_score_smiles(all_generated_smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "76a3cfd3-1d06-411c-b8b6-b14ae48503de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_smiles(validated_smiles, num_top_smiles, filename='top_generated_smiles.txt'):\n",
    "\n",
    "    sorted_smiles = sorted(validated_smiles, key=lambda x: x[1], reverse=True)\n",
    "    top_smiles = sorted_smiles[:num_top_smiles]\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        for smiles, score in top_smiles:\n",
    "            file.write(f\"{smiles}\\n\")\n",
    "\n",
    "save_top_smiles(validated_smiles, 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d15fa67c-24b8-4a99-b5ae-36de982ab96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from resources.utils import canonicalize_smiles, get_Pareto_fronts\n",
    "\n",
    "def load_smiles(filename):\n",
    "    return pd.read_csv(filename, header=None, names=['smiles'])['smiles'].tolist()\n",
    "\n",
    "smiles_real = load_smiles(\"smiles_train.txt\")\n",
    "smiles_gen = load_smiles(\"top_generated_smiles.txt\")\n",
    "\n",
    "sample_size = 10000\n",
    "smiles_real_sample = np.random.choice(smiles_real, sample_size, replace=False)\n",
    "smiles_gen_sample = np.random.choice(smiles_gen, sample_size, replace=False)\n",
    "\n",
    "smiles_real_can = [w for w in canonicalize_smiles(smiles_real_sample) if w is not None]\n",
    "smiles_gen_can = [w for w in canonicalize_smiles(smiles_gen_sample) if w is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9064630f-18f3-4459-933e-11603dd695e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity:  1.0\n"
     ]
    }
   ],
   "source": [
    "validity = len(smiles_gen_can) / len(smiles_gen)\n",
    "print(\"Validity: \", validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a79093b3-3ca9-47e1-a1ea-53c06b026b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniqueness:  0.9995\n"
     ]
    }
   ],
   "source": [
    "smiles_unique = set(smiles_gen_can)\n",
    "uniqueness = len(smiles_unique) / len(smiles_gen)\n",
    "print(\"Uniqueness: \", uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1bdca98c-7956-4822-a8e9-cade6689622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty: 0.9993\n"
     ]
    }
   ],
   "source": [
    "smiles_novel = set(smiles_gen_can) - set(smiles_real_can)\n",
    "novelty = len(smiles_novel) / len(smiles_gen)\n",
    "print(\"Novelty:\", novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7e2d2f9-f0a9-49ac-8907-bf35faff4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fcd\n",
    "ref_model = fcd.load_ref_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b2f32e8-ba3a-4723-84bc-a9a61e2438ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCD:  1.069638603612944\n"
     ]
    }
   ],
   "source": [
    "# Get CHEMBLNET activations of generated molecules \n",
    "act_real = fcd.get_predictions(ref_model, smiles_real_can)\n",
    "act_gen = fcd.get_predictions(ref_model, smiles_gen_can)\n",
    "\n",
    "# Calculate mean and covariance statistics from these activations for both sets\n",
    "mu_real = np.mean(act_real, axis=0)\n",
    "sigma_real = np.cov(act_real.T)\n",
    "\n",
    "mu_gen = np.mean(act_gen, axis=0)\n",
    "sigma_gen = np.cov(act_gen.T)\n",
    "\n",
    "# Calculate the FCD\n",
    "fcd_value = fcd.calculate_frechet_distance(\n",
    "    mu1=mu_real,\n",
    "    mu2=mu_gen, \n",
    "    sigma1=sigma_real,\n",
    "    sigma2=sigma_gen)\n",
    "\n",
    "print('FCD: ', fcd_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f320531-e183-4fa5-9ccf-db88f0864873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
